{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Risk Release Model Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0usE_JOYnYc4"
      },
      "source": [
        "# Need to uninstall these existing versions to upgrade libraries\n",
        "%pip uninstall -y matplotlib\n",
        "%pip uninstall -y plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yx_vIbSoMZk"
      },
      "source": [
        "# click restart runtime (below) after install\n",
        "%pip install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7KfWPN3Bfxb"
      },
      "source": [
        "%pip install --upgrade matplotlib\n",
        "%pip install --upgrade plotly\n",
        "%pip install --upgrade seaborn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6RYIGJIC9Qx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%pip install geopandas\n",
        "import geopandas as gpd\n",
        "import plotly.express as px\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from importlib_metadata import version\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-x-8q3KHMNd"
      },
      "source": [
        "\n",
        "import sklearn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK0rVoAgBS0K"
      },
      "source": [
        "\n",
        "import imblearn \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "from imblearn.pipeline import make_pipeline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oueLobnb-RwM"
      },
      "source": [
        "\n",
        "from google.colab import data_table\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqxQmNbPRORl"
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGw4XY6Xw-df"
      },
      "source": [
        "\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "font = {'size'   : 14}\n",
        "plt.rc('font', **font)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For check current version of libraries\n",
        "def get_version_number(lib):\n",
        "  return float('.'.join(version(lib).split('.')[0:2]))\n"
      ],
      "metadata": {
        "id": "QZ7oRnTvkK7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42qV79-eQUM"
      },
      "source": [
        "# Load and clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A205rlbTRBv3"
      },
      "source": [
        "\n",
        "df = pd.read_csv('../../data/modeling_data/release_risk/filtered_release_risk_modeling_dataset.csv')\n",
        "df = df.drop(['sum_disposal_sites'], axis=1)\n",
        "df = df.dropna()\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aAjW_-0yC2b"
      },
      "source": [
        "# remove columns that contain only zeros\n",
        "df = df.loc[:,list(((df==0).sum()/df.shape[0])!=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SMWG5LJ4PE5"
      },
      "source": [
        "# remove case with inf value\n",
        "df = df.drop(df['pop_density_acs_2018'][(df['pop_density_acs_2018']==np.inf)].index, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvcXE5PqKhTa"
      },
      "source": [
        "# Data prep\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUOMe069Y59f"
      },
      "source": [
        "Xy = df[['response',\n",
        "          'ALAND10',\n",
        "          'AWATER10',\n",
        "          'county',\n",
        "          'places',\n",
        "          'ALL OTHER CONVERTED PAPER PRODUCT MANUFACTURING',\n",
        "          'ALL OTHER MISCELLANEOUS TEXTILE PRODUCT MILLS',\n",
        "          'ARTIFICIAL & SYNTHETIC FIBERS & FILAMENTS MFG',\n",
        "          'BROADWOVEN FABRIC MILLS',\n",
        "          'CARPET & RUG MILLS',\n",
        "          'CORRUGATED & SOLID FIBER BOX MANUFACTURING',\n",
        "          'CURTAIN & LINEN MILLS',\n",
        "          'FABRIC COATING MILLS',\n",
        "          'FIBER YARN & THREAD MILLS',\n",
        "          'FOLDING PAPERBOARD BOX MANUFACTURING',\n",
        "          'KNIT FABRIC MILLS',\n",
        "          \"MEN'S & BOY'S CUT & SEW APPAREL MANUFACTURING\",\n",
        "          'NARROW FABRIC MILLS & SCHIFFLI MACHINE EMBROIDERY',\n",
        "          'NONWOVEN FABRIC MILLS',\n",
        "          'OTHER APPAREL KNITTING MILLS',\n",
        "          'OTHER CUT & SEW APPAREL MANUFACTURING',\n",
        "          'OTHER PAPERBOARD CONTAINER MANUFACTURING',\n",
        "          'PAPER BAG & COATED & TREATED PAPER MANUFACTURING',\n",
        "          'PLASTICS MATERIAL & RESIN MANUFACTURING',\n",
        "          'SANITARY PAPER PRODUCT MANUFACTURING',\n",
        "          'STATIONERY PRODUCT MANUFACTURING',\n",
        "          'TEXTILE & FABRIC FINISHING MILLS',\n",
        "          'TEXTILE BAG & CANVAS MILLS',\n",
        "          \"WOMEN'S GIRLS' & INFANTS' CUT & SEW APPAREL MFG\",         \n",
        "          'sum_firestations',\n",
        "          'sum_industrial_sites',\n",
        "          'pop_density_acs_2018',\n",
        "          'sum_airports',\n",
        "          'sum_army_bases',\n",
        "          'sum_highways',\n",
        "          'AGRICULTURAL',\n",
        "          'COMMERCIAL',\n",
        "          'EXEMPT',\n",
        "          'FOREST',\n",
        "          'INDUSTRIAL',\n",
        "          'MULTIPLE-USE',\n",
        "          'OPEN-SPACE',\n",
        "          'RECREATIONAL',\n",
        "          'RESIDENTIAL']]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzK3LLxW-sfU"
      },
      "source": [
        "\n",
        "print(\"Unique places:\", len(df['places'].unique()))\n",
        "print(\"Unique counties:\", len(df['county'].unique()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uyd51Rl-MmL"
      },
      "source": [
        "# how many cases are non-zero?\n",
        "(Xy>0).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcHFXixY934I"
      },
      "source": [
        "# which variables have low max values\n",
        "(df.describe().T).index[(df.describe().T['max']<4).values]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB3RhpITo6zr"
      },
      "source": [
        "# which variables do not have low max values\n",
        "list((df.describe().T).index[(df.describe().T['max']>=4).values])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMgI_GWDokVJ"
      },
      "source": [
        "\n",
        "# binary encode features with low max values\n",
        "\n",
        "bin_features = ['ALL OTHER CONVERTED PAPER PRODUCT MANUFACTURING',\n",
        "                'ALL OTHER MISCELLANEOUS TEXTILE PRODUCT MILLS',\n",
        "                'ARTIFICIAL & SYNTHETIC FIBERS & FILAMENTS MFG',\n",
        "                'BROADWOVEN FABRIC MILLS',\n",
        "                'CARPET & RUG MILLS',\n",
        "                'CORRUGATED & SOLID FIBER BOX MANUFACTURING',\n",
        "                'CURTAIN & LINEN MILLS',\n",
        "                'FABRIC COATING MILLS',\n",
        "                'FIBER YARN & THREAD MILLS',\n",
        "                'FOLDING PAPERBOARD BOX MANUFACTURING',\n",
        "                'KNIT FABRIC MILLS',\n",
        "                \"MEN'S & BOY'S CUT & SEW APPAREL MANUFACTURING\",\n",
        "                'NARROW FABRIC MILLS & SCHIFFLI MACHINE EMBROIDERY',\n",
        "                'NONWOVEN FABRIC MILLS',\n",
        "                'OTHER APPAREL KNITTING MILLS',\n",
        "                'OTHER CUT & SEW APPAREL MANUFACTURING',\n",
        "                'OTHER PAPERBOARD CONTAINER MANUFACTURING',\n",
        "                'PAPER BAG & COATED & TREATED PAPER MANUFACTURING',\n",
        "                'PLASTICS MATERIAL & RESIN MANUFACTURING',\n",
        "                'SANITARY PAPER PRODUCT MANUFACTURING',\n",
        "                'STATIONERY PRODUCT MANUFACTURING',\n",
        "                'TEXTILE & FABRIC FINISHING MILLS',\n",
        "                'TEXTILE BAG & CANVAS MILLS',\n",
        "                \"WOMEN'S GIRLS' & INFANTS' CUT & SEW APPAREL MFG\"]\n",
        "\n",
        "Xy = df[bin_features]\n",
        "\n",
        "# set value as present True/False for each location\n",
        "Xy = Xy>0\n",
        "Xy = Xy.replace(True, 1)\n",
        "Xy = Xy.replace(False, 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSGmA259xD_-"
      },
      "source": [
        "# attach remaining features\n",
        "\n",
        "continuous_features =  ['ALAND10', \n",
        "                        'AWATER10',\n",
        "                        'AGRICULTURAL',\n",
        "                        'COMMERCIAL',\n",
        "                        'EXEMPT',\n",
        "                        'FOREST',\n",
        "                        'INDUSTRIAL',\n",
        "                        'MULTIPLE-USE',\n",
        "                        'OPEN-SPACE',\n",
        "                        'RECREATIONAL',\n",
        "                        'RESIDENTIAL',\n",
        "                        'sum_firestations',\n",
        "                        'sum_industrial_sites',\n",
        "                        'sum_airports',\n",
        "                        'sum_army_bases',\n",
        "                        'sum_highways']\n",
        "\n",
        "\n",
        "info_vars = ['GEOID10', 'county', 'places', 'INTPTLAT10', 'INTPTLON10']\n",
        "\n",
        "Xy = pd.concat((Xy, df[info_vars]), axis=1)\n",
        "Xy['response'] = df['response']\n",
        "\n",
        "\n",
        "Xy = pd.concat((Xy, df[continuous_features]), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UBUlyXBKizn"
      },
      "source": [
        "# split by response value\n",
        "X_pos = Xy.loc[Xy['response']==1, ].drop('response', axis=1)\n",
        "X_neg = Xy.loc[Xy['response']==0, ].drop('response', axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBQjwihQch6C"
      },
      "source": [
        "\n",
        "# Create table showing the proportion of features (binary encoded) are present in each response class\n",
        "bin_summary_df = pd.DataFrame()\n",
        "bin_summary_df['feature'] = X_pos[bin_features].columns\n",
        "bin_summary_df['pos_pct'] = X_pos[bin_features].sum().values / X_pos.shape[0] * 100\n",
        "bin_summary_df['neg_pct'] = X_neg[bin_features].sum().values / X_neg.shape[0] * 100\n",
        "\n",
        "bin_summary_df['diff'] = bin_summary_df['pos_pct'] - bin_summary_df['neg_pct']\n",
        "bin_summary_df['abs_diff'] = np.abs(bin_summary_df['diff'])\n",
        "\n",
        "bin_summary_df = bin_summary_df.sort_values('diff', ascending=False)\n",
        "bin_summary_df = bin_summary_df.reset_index(drop=True)\n",
        "\n",
        "display(bin_summary_df)\n",
        "\n",
        "bin_summary_df = bin_summary_df.sort_values('abs_diff', ascending=True)\n",
        "bin_summary_df.loc[bin_summary_df['diff'] > 0, 'pos_diff'] = bin_summary_df['diff'] \n",
        "bin_summary_df.loc[bin_summary_df['diff'] < 0, 'neg_diff'] = bin_summary_df['diff'] \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os2kC2riCl9q"
      },
      "source": [
        "# plot differences in proportions of features present by class\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,10))\n",
        "\n",
        "labels = list(bin_summary_df['feature'])\n",
        "label_ticks = np.arange(len(labels))\n",
        "bar_width = .4\n",
        "\n",
        "bars = ax.barh(label_ticks, bin_summary_df['pos_diff'], label='Response=1', color='red')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars, np.round(bin_summary_df['abs_diff'],1), padding=3)\n",
        "\n",
        "bars = ax.barh(label_ticks, bin_summary_df['neg_diff'], label='Response=0', color='blue')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars,  np.round(bin_summary_df['abs_diff'],1), padding=3)\n",
        "\n",
        "ax.set_xlim(-10, 10)\n",
        "ax.set_xlabel('Difference between percentages of features present, by response class')\n",
        "\n",
        "ax.set_title('Percent of locations containing each feature')\n",
        "ax.set_yticks(label_ticks)\n",
        "ax.set_yticklabels(labels)\n",
        "\n",
        "ax.vlines(0, -1, len(label_ticks), color='black', linewidth=.5, alpha=.75)\n",
        "ax.set_ylim(-1, len(label_ticks))\n",
        "\n",
        "ax.grid(True, axis='x', linestyle=':', linewidth=.5, color='green', )\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot just the main feature percent differences\n",
        "\n",
        "bin_summary_plot = bin_summary_df.loc[bin_summary_df['pos_diff']>=.5,]\n",
        "\n",
        "labels = list(bin_summary_plot['feature'])\n",
        "label_ticks = np.arange(len(labels))\n",
        "bar_width = .4\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,10))\n",
        "\n",
        "bars = ax.barh(label_ticks, bin_summary_plot['pos_diff'], label='Response=1')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars, np.round(bin_summary_plot['abs_diff'],1), padding=3, fontsize=22)\n",
        "\n",
        "ax.set_xlim(-0, 10)\n",
        "ax.set_xlabel('Percent of locations containing each feature', fontdict={'fontsize':22})\n",
        "\n",
        "ax.set_title('Percent Higher Prevelance in PFAS Release Areas', fontdict={'fontsize':22})\n",
        "\n",
        "ax.set_yticks(label_ticks)\n",
        "ax.set_yticklabels(labels, fontsize=18)\n",
        "\n",
        "ax.vlines(0, -1, len(label_ticks), color='black', linewidth=.5, alpha=.75)\n",
        "ax.set_ylim(-1, len(label_ticks))\n",
        "\n",
        "ax.grid(True, axis='x', linestyle=':', linewidth=.5, color='green', )\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WFsRQ9u7btFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtuNDyOcEftu"
      },
      "source": [
        "\n",
        "# Create table showing mean differences and z-score differences for continuous features in each response class\n",
        "\n",
        "# create summary table \n",
        "continous_summary_df = pd.DataFrame()\n",
        "continous_summary_df['feature'] = X_pos[continuous_features].columns\n",
        "continous_summary_df['Xy_mean'] = Xy[continuous_features].mean().values\n",
        "continous_summary_df['Xy_std'] = Xy[continuous_features].std().values\n",
        "\n",
        "continous_summary_df['pos_mean'] = X_pos[continuous_features].mean().values\n",
        "continous_summary_df['neg_mean'] = X_neg[continuous_features].mean().values\n",
        "\n",
        "continous_summary_df['pos_z'] = (X_pos[continuous_features].mean().values - continous_summary_df['Xy_mean']) / continous_summary_df['Xy_std']\n",
        "continous_summary_df['neg_z'] = (X_neg[continuous_features].mean().values - continous_summary_df['Xy_mean']) / continous_summary_df['Xy_std']\n",
        "\n",
        "continous_summary_df['diff'] = continous_summary_df['pos_z'] - continous_summary_df['neg_z']\n",
        "continous_summary_df['abs_diff'] = np.abs(continous_summary_df['diff'])\n",
        "\n",
        "continous_summary_df = continous_summary_df.sort_values('abs_diff', ascending=False)\n",
        "continous_summary_df = continous_summary_df.reset_index(drop=True)\n",
        "\n",
        "display(continous_summary_df)\n",
        "\n",
        "continous_summary_df = continous_summary_df.sort_values('abs_diff', ascending=True)\n",
        "continous_summary_df.loc[continous_summary_df['diff'] > 0, 'pos_diff'] = continous_summary_df['diff'] \n",
        "continous_summary_df.loc[continous_summary_df['diff'] < 0, 'neg_diff'] = continous_summary_df['diff'] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMFH8_HbEviF"
      },
      "source": [
        "# plot differences in z-scores for continous features by class\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,10))\n",
        "\n",
        "labels = list(continous_summary_df['feature'])\n",
        "label_ticks = np.arange(len(labels))\n",
        "\n",
        "bar_width = .4\n",
        "\n",
        "bars = ax.barh(label_ticks, continous_summary_df['pos_diff'], label='Response=1', color='red')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars, np.round(continous_summary_df['abs_diff'],1), padding=3)\n",
        "\n",
        "bars = ax.barh(label_ticks, continous_summary_df['neg_diff'], label='Response=0', color='blue')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars,  np.round(continous_summary_df['abs_diff'],1), padding=3)\n",
        "\n",
        "ax.set_xlim(-1, 1)\n",
        "ax.set_title('Difference between z-scores of values for each response class (of overall mean)')\n",
        "\n",
        "ax.set_xlabel('z-score difference')\n",
        "ax.set_yticks(label_ticks)\n",
        "ax.set_yticklabels(labels)\n",
        "\n",
        "ax.vlines(0, -1, len(label_ticks), color='black', linewidth=.5, alpha=.75)\n",
        "ax.set_ylim(-1, len(label_ticks))\n",
        "\n",
        "ax.grid(True, axis='x', linestyle=':', linewidth=.5, color='green', )\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8AXdzM0Lgkr"
      },
      "source": [
        "# plot just the main feature z score differences, for presentation\n",
        "\n",
        "continous_summary_plot = continous_summary_df.loc[continous_summary_df['abs_diff'] > 0.1,]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,10))\n",
        "\n",
        "labels = list(continous_summary_plot['feature'])\n",
        "label_ticks = np.arange(len(labels))\n",
        "\n",
        "bar_width = .4\n",
        "\n",
        "bars = ax.barh(label_ticks, continous_summary_plot['pos_diff'], label='PFAS Release Area', color='red')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars, np.round(continous_summary_plot['abs_diff'],1), padding=3)\n",
        "\n",
        "bars = ax.barh(label_ticks, continous_summary_plot['neg_diff'], label='No Documented Release', color='blue')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  ax.bar_label(bars,  np.round(continous_summary_plot['abs_diff'],1), padding=3)\n",
        "\n",
        "ax.set_xlim(-1, 1)\n",
        "ax.set_title('Relative difference in average values for area characteristics\\n', fontdict={'fontsize':22})\n",
        "ax.set_xlabel('z-score difference', fontsize=18)\n",
        "\n",
        "ax.set_yticks(label_ticks)\n",
        "ax.set_yticklabels(labels, fontsize=18)\n",
        "\n",
        "ax.vlines(0, -1, len(label_ticks), color='black', linewidth=.5, alpha=.75)\n",
        "ax.set_ylim(-1, len(label_ticks))\n",
        "\n",
        "ax.grid(True, axis='x', linestyle=':', linewidth=.5, color='green', )\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(-0.03, -.1), loc='upper left', fontsize=22, markerscale=1.5);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA5vCe2AFQju"
      },
      "source": [
        "\n",
        "important_bin_features = list(bin_summary_df.loc[bin_summary_df['diff']>0, 'feature'].values)\n",
        "\n",
        "print('\\nFeatures with positive difference in proportions > 0:')\n",
        "display(important_bin_features)\n",
        "\n",
        "important_continous_features = list(continous_summary_df.loc[continous_summary_df['abs_diff']>.1, 'feature'].values)\n",
        "\n",
        "# drop total land area variable to avoid bias due to census place size\n",
        "if 'ALAND10' in important_continous_features:\n",
        "  important_continous_features.remove('ALAND10')\n",
        "\n",
        "print('\\nFeatures with abs difference in mean z-scores > 0.1:')\n",
        "display(important_continous_features)\n",
        "\n",
        "print('\\nImportant features selected for modeling')\n",
        "important_features = important_bin_features + important_continous_features\n",
        "\n",
        "display(important_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjBAQVKAsTFJ"
      },
      "source": [
        "#Aggregate by location grouping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9EbjroPshfD"
      },
      "source": [
        "\n",
        "\n",
        "Xy = df[important_features]\n",
        "Xy['places'] = df['places']\n",
        "Xy['response'] = df['response']\n",
        "\n",
        "Xy_aggr = Xy.groupby(['places'], as_index=False).agg('mean')\n",
        "\n",
        "Xy_aggr.loc[Xy_aggr['response']>0, 'response'] = 1\n",
        "X = Xy_aggr[important_features]\n",
        "y = Xy_aggr['response']\n",
        "X.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNF_75hd5Duz"
      },
      "source": [
        "# Train/test split size comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksfP9w4xx2wq"
      },
      "source": [
        "\n",
        "# train default models and return repeated cross validation results over train/test splits of increasing size (with same random state).\n",
        "\n",
        "seed = 42\n",
        "k_folds = 5\n",
        "n_repeats = 10 \n",
        "\n",
        "model_list = [LogisticRegression(random_state=seed),\n",
        "              KNeighborsClassifier(),\n",
        "              RandomForestClassifier(random_state=seed)]\n",
        "\n",
        "results = []\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "for train_size in np.arange(.5, .82, .02):\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                      stratify=y,\n",
        "                                                      random_state=seed,\n",
        "                                                      train_size=train_size)\n",
        "\n",
        "  for clf in model_list:\n",
        "\n",
        "      model_str = str(type(clf))\n",
        "      print('Evaluating: {},  train_size: {:.2f} '. format(model_str, train_size))\n",
        "\n",
        "      model = make_pipeline(\n",
        "                            StandardScaler(), \n",
        "                            SMOTE(random_state=seed),\n",
        "                            RandomUnderSampler(random_state=seed),\n",
        "                            clf)\n",
        "      \n",
        "      rkf = RepeatedStratifiedKFold(n_splits=k_folds, \n",
        "                                    n_repeats=n_repeats,\n",
        "                                    random_state=seed)\n",
        "\n",
        "      cv_results = cross_validate(model, \n",
        "                                  X_train, y_train, \n",
        "                                  cv=rkf,\n",
        "                                  scoring=['accuracy','recall', 'precision', 'f1', 'roc_auc'])\n",
        "\n",
        "      results.append({'model':model_str, \n",
        "                      'train_size':train_size,\n",
        "                      'cv_accuracy':cv_results['test_accuracy'].mean(),  \n",
        "                      'cv_precision':cv_results['test_precision'].mean(),                      \n",
        "                      'cv_recall':cv_results['test_recall'].mean(),\n",
        "                      'cv_f1':cv_results['test_f1'].mean(),\n",
        "                      'cv_ROC':cv_results['test_roc_auc'].mean()})\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrUlkoqLTzM3"
      },
      "source": [
        "\n",
        "# plot train size results comparison\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "g = sns.FacetGrid(data=results_df.melt(['model', 'train_size']), col='variable', hue='model', height=4, aspect=.9)\n",
        "g.map_dataframe(sns.lineplot, x='train_size', y='value')\n",
        "plt.ylim(0,1)\n",
        "plt.xticks(np.arange(.5, .85, .1))\n",
        "plt.legend(bbox_to_anchor=(-4.5, -.4), loc='upper left'); \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcVlrNYc4_Ch"
      },
      "source": [
        "# Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKaSPGE6QYUv"
      },
      "source": [
        "\n",
        "seed = 42\n",
        "\n",
        "train_size = .65\n",
        "k_folds = 5\n",
        "prob_threshold = .5\n",
        "n_repeats = 50\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                        stratify=y,\n",
        "                                        train_size=train_size,\n",
        "                                        random_state=seed\n",
        "                                        )\n",
        "\n",
        "model_list = [\n",
        "              LogisticRegression(random_state=seed),\n",
        "              KNeighborsClassifier(),\n",
        "              RandomForestClassifier(random_state=seed)\n",
        "              ]\n",
        "\n",
        "\n",
        "params = [\n",
        "          {'smote__sampling_strategy': [0.5, 0.6, 0.8, 1.0],\n",
        "          'logisticregression__C': (0.01, 0.1, 0.2, 0.5, 0.8, 1, 2, 5, 10),\n",
        "          'logisticregression__solver': ['lbfgs', 'liblinear', 'sag'],\n",
        "          'logisticregression__penalty': ['l1', 'l2'],\n",
        "          'logisticregression__class_weight': ['balanced', None],},\n",
        "\n",
        "          {'smote__sampling_strategy': [0.5, 0.6, 0.8, 1.0],\n",
        "           'kneighborsclassifier__n_neighbors': np.arange(1,25,1),},\n",
        "          \n",
        "          {'smote__sampling_strategy': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "          'randomforestclassifier__n_estimators': [5,10,20,50], #,100,500],\n",
        "          'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
        "          'randomforestclassifier__max_depth': [1, 2, 3, 4, 5],\n",
        "          'randomforestclassifier__min_samples_split': [2, 3, 5, 10],\n",
        "          'randomforestclassifier__min_samples_leaf': [1, 2, 4, 8],\n",
        "           }]\n",
        "                     \n",
        "\n",
        "models=[]\n",
        "best_params = []\n",
        "model_results_df = pd.DataFrame()\n",
        "\n",
        "print('\\ntrain_size:', train_size,\n",
        "      '\\nk_folds:', k_folds,\n",
        "      '\\nn_repeats:', n_repeats)\n",
        "      \n",
        "\n",
        "for clf, param in zip(model_list, params):\n",
        "  \n",
        "    model_str = str(type(clf))\n",
        "    print('\\nmodel: ',model_str)\n",
        "\n",
        "    model = make_pipeline(\n",
        "                          StandardScaler(),\n",
        "                          SMOTE(random_state=seed),\n",
        "                          RandomUnderSampler(random_state=seed),\n",
        "                          clf)\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=k_folds, \n",
        "                        random_state=seed, \n",
        "                        shuffle=True)\n",
        "    \n",
        "    rkf = RepeatedStratifiedKFold(n_splits=k_folds, \n",
        "                                  n_repeats=n_repeats,\n",
        "                                  random_state=seed\n",
        "                                  )\n",
        "\n",
        "    # perform search of parameters for each model, saving best configuration to model object.\n",
        "    model = RandomizedSearchCV(model,\n",
        "                               param,\n",
        "                               cv=kf,\n",
        "                               # random_state=seed,\n",
        "                               n_iter=10,\n",
        "                               scoring='recall')\n",
        "    \n",
        "    # rerun cross validation using best params and repeated stratified k-fold to control for model instability due to limited training data\n",
        "    cv_results = cross_validate(model, \n",
        "                                X_train, y_train, \n",
        "                                cv=rkf,\n",
        "                                scoring=['precision', 'accuracy', 'recall', 'f1', 'roc_auc']\n",
        "                                )\n",
        "\n",
        "    # output results\n",
        "    print('cv_accuracy   mean: {:.2f} | sd: {:.2f}'. format(cv_results['test_accuracy'][~np.isnan(cv_results['test_accuracy'])].mean(), \n",
        "                                                            cv_results['test_accuracy'][~np.isnan(cv_results['test_accuracy'])].std()))\n",
        "    \n",
        "    print('cv_recall     mean: {:.2f} | sd: {:.2f}'. format(cv_results['test_recall'][~np.isnan(cv_results['test_recall'])].mean(), \n",
        "                                                            cv_results['test_recall'][~np.isnan(cv_results['test_recall'])].std()))\n",
        "    \n",
        "    print('cv_precision  mean: {:.2f} | sd: {:.2f}'. format(cv_results['test_precision'][~np.isnan(cv_results['test_precision'])].mean(), \n",
        "                                                            cv_results['test_precision'][~np.isnan(cv_results['test_precision'])].std()))\n",
        "    \n",
        "    print('cv_f1         mean: {:.2f} | sd: {:.2f}'. format(cv_results['test_f1'][~np.isnan(cv_results['test_f1'])].mean(), \n",
        "                                                            cv_results['test_f1'][~np.isnan(cv_results['test_f1'])].std()))\n",
        "    \n",
        "    print('cv_ROC        mean: {:.2f} | sd: {:.2f}'. format(cv_results['test_roc_auc'][~np.isnan(cv_results['test_roc_auc'])].mean(), \n",
        "                                                            cv_results['test_roc_auc'][~np.isnan(cv_results['test_roc_auc'])].std()))\n",
        "\n",
        "    # save cv results for comparison of models and creation of ensembles\n",
        "    cv_results_df = pd.DataFrame()\n",
        "    cv_results_df['model'] = ''\n",
        "    cv_results_df['test_accuracy'] = cv_results['test_accuracy']\n",
        "    cv_results_df['test_recall'] = cv_results['test_recall']\n",
        "    cv_results_df['test_precision'] = cv_results['test_precision']\n",
        "    cv_results_df['test_f1'] = cv_results['test_f1']\n",
        "    cv_results_df['test_roc_auc'] = cv_results['test_roc_auc']\n",
        "    cv_results_df['model'] = model_str\n",
        "\n",
        "    model_results_df = model_results_df.append(cv_results_df, ignore_index=True)\n",
        "    model_results_df.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # save fitted models with best params for testing\n",
        "    model = model.fit(X_train, y_train)\n",
        "    models.append(model)\n",
        "\n",
        "    print('\\nbest params:')\n",
        "    print(model.best_params_)\n",
        "    best_params.append(model.best_params_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vKIjcvgfQnu"
      },
      "source": [
        "\n",
        "# Shorten model names\n",
        "model_results_df.loc[model_results_df['model']==\"<class 'sklearn.linear_model._logistic.LogisticRegression'>\", 'model'] = 'Logistic Regression'\n",
        "model_results_df.loc[model_results_df['model']==\"<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\", 'model'] = 'K-Nearest Neighbors'\n",
        "model_results_df.loc[model_results_df['model']==\"<class 'sklearn.ensemble._forest.RandomForestClassifier'>\", 'model'] = 'Random Forest'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CV results table\n",
        "np.round(model_results_df.groupby('model', as_index=False).agg('mean').sort_values('test_recall', ascending=False),3)\n"
      ],
      "metadata": {
        "id": "fXrkPFbJWweK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elt6xmhdub--"
      },
      "source": [
        "\n",
        "# plot cross validation scoring results\n",
        "\n",
        "font = {'size'   : 14}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "fig = plt.figure(figsize=(9,6))\n",
        "\n",
        "sns.boxplot(data=model_results_df.melt('model'), y='value', x='variable', hue='model', width=.7,\n",
        "            fliersize=0, whis=1.5, linewidth=.5\n",
        "            )\n",
        "\n",
        "plt.xticks(ticks=np.arange(0, 5), labels=['Accuracy', 'Recall', 'Precision', 'F1', 'ROC AUC'], \n",
        "          #  rotation=45, rotation_mode='anchor', ha='right'\n",
        "           )\n",
        "\n",
        "plt.ylabel('Score')\n",
        "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
        "plt.title('K-Fold Cross Validation Metric Results\\n');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict on Test Set"
      ],
      "metadata": {
        "id": "luOcld6akqFt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U4jpJ8F-ltb"
      },
      "source": [
        "\n",
        "# get predictions from models for test set\n",
        "\n",
        "test_pred_df = pd.DataFrame()\n",
        "test_pred_df['response'] = y_test\n",
        "test_prob_df = pd.DataFrame()\n",
        "test_prob_df['response'] = y_test\n",
        "\n",
        "model_names = ['LR_prob', 'KNN_prob', 'RF_prob']\n",
        "\n",
        "model_list_str = [str(type(m)) for m in model_list]\n",
        "\n",
        "for i,model in enumerate(models):\n",
        "\n",
        "    y_prob = model.predict_proba(X_test)[:,1]\n",
        "    y_pred = y_prob > prob_threshold\n",
        "\n",
        "    test_pred_df[model_names[i]] = y_prob\n",
        "\n",
        "    print(model_list_str[i])\n",
        "    print('test accuracy:  {:.2f}'. format(accuracy_score(y_test, y_pred)))\n",
        "    print('test recall:    {:.2f}'. format(recall_score(y_test, y_pred)))\n",
        "    print('test precision: {:.2f}'. format(precision_score(y_test, y_pred)))\n",
        "    print('test roc_auc:   {:.2f}'. format(roc_auc_score(y_test, y_pred)))\n",
        "    print('test f1:        {:.2f}'. format(f1_score(y_test, y_pred)))\n",
        "\n",
        "    # print('\\nClassification report : \\n', \n",
        "    #       classification_report(y_test, y_pred, labels=[1,0]))\n",
        "\n",
        "    tp, fn, fp, tn = confusion_matrix(y_test, y_pred, labels=[1,0]).reshape(-1)\n",
        "    print('\\nOutcome values :',\n",
        "          '\\ntp:', tp, \n",
        "          '\\nfn:', fn, \n",
        "          '\\nfp:', fp, \n",
        "          '\\ntn:', tn)\n",
        "    print('\\n------------\\n')\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV_csSlOHIjQ"
      },
      "source": [
        "\n",
        "# predicted class = 1 where probability > prob_threshold \n",
        "\n",
        "test_pred_df.loc[test_pred_df['LR_prob'] > prob_threshold, 'LR_pred'] = 1\n",
        "test_pred_df.loc[test_pred_df['LR_prob'] <= prob_threshold, 'LR_pred'] = 0\n",
        "\n",
        "test_pred_df.loc[test_pred_df['KNN_prob'] > prob_threshold, 'KNN_pred'] = 1\n",
        "test_pred_df.loc[test_pred_df['KNN_prob'] <= prob_threshold, 'KNN_pred'] = 0\n",
        "\n",
        "test_pred_df.loc[test_pred_df['RF_prob'] > prob_threshold, 'RF_pred'] = 1\n",
        "test_pred_df.loc[test_pred_df['RF_prob'] <= prob_threshold, 'RF_pred'] = 0\n",
        "\n",
        "\n",
        "# compute soft voting ensemble probability and prediction\n",
        "test_pred_df['prob_avg'] = test_pred_df[['LR_prob', \n",
        "                                         'KNN_prob', \n",
        "                                         'RF_prob']].mean(axis=1)\n",
        "\n",
        "test_pred_df.loc[test_pred_df['prob_avg'] > prob_threshold, 'soft_vote'] = 1\n",
        "test_pred_df.loc[test_pred_df['prob_avg'] <= prob_threshold, 'soft_vote'] = 0\n",
        "\n",
        "# compute hard voting ensemble prediction \n",
        "test_pred_df['hard_vote'] = test_pred_df[['LR_pred', \n",
        "                                          'KNN_pred', \n",
        "                                          'RF_pred']].mode(axis=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoEu-cSul4-m"
      },
      "source": [
        "\n",
        "# categorize prediction outcomes\n",
        "\n",
        "pred_df = test_pred_df.drop(['LR_prob', 'KNN_prob', 'RF_prob', 'prob_avg'], axis=1)\n",
        "\n",
        "pred_df = pred_df.melt('response', var_name='model', value_name='pred')\n",
        "\n",
        "pred_df.loc[(pred_df['pred']==1) & (pred_df['response']==1), 'pred_result'] = 'True Positive'\n",
        "pred_df.loc[(pred_df['pred']==0)  & (pred_df['response']==1), 'pred_result'] = 'False Negative'\n",
        "pred_df.loc[(pred_df['pred']==1) & (pred_df['response']==0), 'pred_result'] = 'False Positive'\n",
        "pred_df.loc[(pred_df['pred']==0)  & (pred_df['response']==0), 'pred_result'] = 'True Negative'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib2VQeiMnQoc"
      },
      "source": [
        "\n",
        "# sum up prediction results in crosstab table\n",
        "pred_ct = pd.crosstab(pred_df['model'],pred_df['pred_result'])\n",
        "\n",
        "pred_ct['total'] = pred_ct[['True Positive', 'True Negative', 'False Positive', 'False Negative']].sum(axis=1)\n",
        "pred_ct['correct'] = pred_ct['True Positive'] + pred_ct['True Negative']\n",
        "\n",
        "pred_ct['Accuracy'] = pred_ct['correct'] / pred_ct['total']\n",
        "pred_ct['Recall'] = pred_ct['True Positive'] / (pred_ct['True Positive'] + pred_ct['False Negative'])\n",
        "pred_ct['Precision'] = pred_ct['True Positive'] / (pred_ct['True Positive'] + pred_ct['False Positive'])\n",
        "pred_ct['F1'] = 2 * (pred_ct['Recall'] * pred_ct['Precision']) / (pred_ct['Recall'] + pred_ct['Precision'])\n",
        "\n",
        "pred_ct = pred_ct.reset_index()\n",
        "pred_ct\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ct.loc[pred_ct['model']=='KNN_pred', 'Model name'] = 'KNN'\n",
        "pred_ct.loc[pred_ct['model']=='LR_pred', 'Model name'] = 'Logistic Regression'\n",
        "pred_ct.loc[pred_ct['model']=='RF_pred', 'Model name'] = 'Random Forest'\n",
        "pred_ct.loc[pred_ct['model']=='soft_vote', 'Model name'] = 'Ensemble - Hard Vote'\n",
        "pred_ct.loc[pred_ct['model']=='hard_vote', 'Model name'] = 'Ensemble - Soft Vote'"
      ],
      "metadata": {
        "id": "QZHbWh6tfNcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqDgwUTUn4Kc"
      },
      "source": [
        "\n",
        "# plot model evaluation results\n",
        "pred_results = pred_ct[['Model name', 'Accuracy', 'Recall', 'Precision', 'F1']].melt('Model name', var_name='Metric', value_name='Score')\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "ax = sns.barplot(data=pred_results, x='Metric', y='Score', hue='Model name')\n",
        "plt.ylim(0,1)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
        "plt.title('Model test score comparisons\\n')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqlZ_zYzwH8U"
      },
      "source": [
        "\n",
        "# Print ensemble results\n",
        "\n",
        "# Soft voting\n",
        "print('\\nSoft voting:')\n",
        "print(classification_report(test_pred_df['response'], test_pred_df['soft_vote'], labels=[1,0]))\n",
        "\n",
        "tp, fn, fp, tn = confusion_matrix(test_pred_df['response'], test_pred_df['soft_vote'], labels=[1,0]).reshape(-1)\n",
        "print('\\nOutcome values :',\n",
        "      '\\ntp:', tp, \n",
        "      '\\nfn:', fn, \n",
        "      '\\nfp:', fp, \n",
        "      '\\ntn:', tn)\n",
        "\n",
        "# Hard voting\n",
        "print('\\nHard voting:')\n",
        "print(classification_report(test_pred_df['response'], test_pred_df['hard_vote'], labels=[1,0]))\n",
        "\n",
        "tp, fn, fp, tn = confusion_matrix(test_pred_df['response'], test_pred_df['hard_vote'], labels=[1,0]).reshape(-1)\n",
        "print('\\nOutcome values :',\n",
        "      '\\ntp:', tp, \n",
        "      '\\nfn:', fn, \n",
        "      '\\nfp:', fp, \n",
        "      '\\ntn:', tn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYvQ4glT5WoC"
      },
      "source": [
        "# Predict on Entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meKul0Q7uyKI"
      },
      "source": [
        "\n",
        "# Run trained models using best params to predict on the entire dataset.\n",
        "\n",
        "full_pred_df = Xy_aggr.copy()\n",
        "full_pred_df = full_pred_df[['places','response']]\n",
        "\n",
        "model_names = ['LR_prob', 'KNN_prob', 'RF_prob']\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "\n",
        "    # model = model.fit(X_train, y_train)\n",
        "    y_prob = model.predict_proba(X)[:,1]\n",
        "    y_pred = y_prob > prob_threshold\n",
        "\n",
        "    full_pred_df[model_names[i]] = y_prob\n",
        "\n",
        "    print(model_names[i])\n",
        "    print('test accuracy:  {:.2f}'. format(accuracy_score(y, y_pred)))\n",
        "    print('test recall:    {:.2f}'. format(recall_score(y, y_pred)))\n",
        "    print('test precision: {:.2f}'. format(precision_score(y, y_pred)))\n",
        "    print('test roc_auc:   {:.2f}'. format(roc_auc_score(y, y_pred)))\n",
        "    print('test f1:        {:.2f}'. format(f1_score(y, y_pred)))\n",
        "\n",
        "    # print('\\nClassification report : \\n', \n",
        "    #       classification_report(y_test, y_pred, labels=[1,0]))\n",
        "\n",
        "    tp, fn, fp, tn = confusion_matrix(y, y_pred, labels=[1,0]).reshape(-1)\n",
        "    print('\\nOutcome values :',\n",
        "          '\\ntp:', tp, \n",
        "          '\\nfn:', fn, \n",
        "          '\\nfp:', fp, \n",
        "          '\\ntn:', tn)\n",
        "    print('\\n------------\\n')\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAD3COPx4rsq"
      },
      "source": [
        "\n",
        "# predicted class = 1 where probability > prob_threshold \n",
        "\n",
        "full_pred_df.loc[full_pred_df['LR_prob'] > prob_threshold, 'LR_pred'] = 1\n",
        "full_pred_df.loc[full_pred_df['LR_prob'] <= prob_threshold, 'LR_pred'] = 0\n",
        "\n",
        "full_pred_df.loc[full_pred_df['KNN_prob'] > prob_threshold, 'KNN_pred'] = 1\n",
        "full_pred_df.loc[full_pred_df['KNN_prob'] <= prob_threshold, 'KNN_pred'] = 0\n",
        "\n",
        "full_pred_df.loc[full_pred_df['RF_prob'] > prob_threshold, 'RF_pred'] = 1\n",
        "full_pred_df.loc[full_pred_df['RF_prob'] <= prob_threshold, 'RF_pred'] = 0\n",
        "\n",
        "\n",
        "# compute soft voting ensemble probability and prediction\n",
        "full_pred_df['prob_avg'] = full_pred_df[['LR_prob', \n",
        "                                         'KNN_prob', \n",
        "                                         'RF_prob']].mean(axis=1)\n",
        "full_pred_df.loc[full_pred_df['prob_avg'] > prob_threshold, 'soft_vote'] = 1\n",
        "full_pred_df.loc[full_pred_df['prob_avg'] <= prob_threshold, 'soft_vote'] = 0\n",
        "\n",
        "# compute hard voting ensemble prediction \n",
        "full_pred_df['hard_vote'] = full_pred_df[['LR_pred','KNN_pred', 'RF_pred']].mode(axis=1)\n",
        "full_pred_df['hard_vote_prob'] = full_pred_df['hard_vote']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1V8Djg9gzv"
      },
      "source": [
        "\n",
        "# select best model to use for final predictions\n",
        "\n",
        "# final_model_pred = 'LR_pred'\n",
        "# final_model_prob = 'LR_prob'\n",
        "\n",
        "# final_model_pred = 'KNN_pred'\n",
        "# final_model_prob = 'KNN_prob'\n",
        "\n",
        "final_model_pred = 'RF_pred'\n",
        "final_model_prob = 'RF_prob'\n",
        "\n",
        "# final_model_pred = 'soft_vote'\n",
        "# final_model_prob = 'prob_avg'\n",
        "\n",
        "# final_model_pred = 'hard_vote'\n",
        "# final_model_prob = 'hard_vote_prob'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT20rKID4rsr"
      },
      "source": [
        "\n",
        "# categorize prediction outcomes\n",
        "\n",
        "pred_df = full_pred_df[['places', 'response', final_model_pred, final_model_prob]]\n",
        "pred_df = pred_df.rename(columns={final_model_pred:'pred', final_model_prob:'prob'})\n",
        "\n",
        "pred_df.loc[(pred_df['pred']==1) & (pred_df['response']==1), 'pred_result'] = 'True Positive'\n",
        "pred_df.loc[(pred_df['pred']==0)  & (pred_df['response']==1), 'pred_result'] = 'False Negative'\n",
        "pred_df.loc[(pred_df['pred']==1) & (pred_df['response']==0), 'pred_result'] = 'False Positive'\n",
        "pred_df.loc[(pred_df['pred']==0)  & (pred_df['response']==0), 'pred_result'] = 'True Negative'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(pred_df['response'], pred_df['pred']))"
      ],
      "metadata": {
        "id": "ZRBi07OtjiD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhdF35i6_zI5"
      },
      "source": [
        "\n",
        "# plot prediction outcome totals \n",
        "\n",
        "ax = sns.countplot(data=pred_df, x='pred_result')\n",
        "if get_version_number('matplotlib') > 3.5:\n",
        "  plt.bar_label(ax.containers[0])\n",
        "plt.ylim(0,40)\n",
        "plt.xticks(rotation=45, rotation_mode='anchor', ha='right')\n",
        "plt.xlabel('')\n",
        "plt.title('Prediction Outcomes for Full Dataset\\n')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zHF1e5IBUFr"
      },
      "source": [
        "\n",
        "# plot prediction results \n",
        "\n",
        "pred_df = pred_df.sort_values('prob', ascending=True)\n",
        "pred_df = pred_df.reset_index(drop=True)\n",
        "\n",
        "font = {'size'   : 14}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "pal = {'True Positive': 'green',\n",
        "       'False Positive':'red',\n",
        "       'True Negative': 'blue',\n",
        "       'False Negative':'#fd8d3c'}\n",
        "\n",
        "sns.scatterplot(pred_df.index, pred_df['prob'], hue=pred_df['pred_result'], palette=pal)\n",
        "\n",
        "plt.ylim(0,1), \n",
        "plt.yticks(np.arange(0, 1.1, .1))\n",
        "plt.grid(which='major', axis='y')\n",
        "\n",
        "plt.xlabel('Locations - ordered index')\n",
        "plt.ylabel('Predicted Probability')\n",
        "plt.title('Model prediction outcomes for all locations')\n",
        "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left');\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Analysis"
      ],
      "metadata": {
        "id": "p3ZUzN25WSZW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpbiIMWQf4fo"
      },
      "source": [
        "\n",
        "# get RF importance rankings for features\n",
        "\n",
        "train_size = .6\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "n_loops = 20\n",
        "importance_df = pd.DataFrame()\n",
        "\n",
        "for i in range(n_loops):\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, \n",
        "                                                      y,\n",
        "                                                      train_size=train_size)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    clf = clf.fit(scaler.fit_transform(X_train), y_train)\n",
        "    importance = clf.feature_importances_\n",
        "    importance_df = pd.concat([importance_df, pd.Series(importance)], axis=1)\n",
        "\n",
        "importance_df['mean_importance'] = importance_df.mean(axis=1)\n",
        "importance_df['feature'] = X_train.columns\n",
        "importance_df[\"rank\"] = importance_df['mean_importance'].rank(ascending = False)\n",
        "importance_df = importance_df[['feature','mean_importance', 'rank']].sort_values('rank', axis=0, ascending=False)\n",
        "importance_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "plt.figure(figsize=(8, 16))\n",
        "plt.barh(importance_df['feature'], \n",
        "         importance_df['mean_importance'])\n",
        "plt.title('mean importance value from RF')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDhFQoKJkZe"
      },
      "source": [
        "#Chloropleth Maps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFslxoot87lJ"
      },
      "source": [
        "\n",
        "# load geopandas dataframe file\n",
        "cbg_gdf = gpd.read_file('../../data/tl_2010_25_bg10.zip')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7eQiQ9oIhj7"
      },
      "source": [
        "\n",
        "# dissolve geopandas df geometry to places level\n",
        "cbg_gdf['places'] = cbg_gdf['GEOID10'].str[:7]\n",
        "places_gdf = cbg_gdf.dissolve(by='places').reset_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lHrcnPVcNV"
      },
      "source": [
        "\n",
        "# add location data to predictions\n",
        "\n",
        "model_locations = Xy_aggr[['places']]\n",
        "\n",
        "model_locations.loc[X_train.index, 'train_test'] = 'Training Set'\n",
        "model_locations.loc[X_test.index, 'train_test'] = 'Test Set'\n",
        "\n",
        "model_locations['places'] = model_locations['places'].astype(str)\n",
        "pred_df['places'] = pred_df['places'].astype(str)\n",
        "\n",
        "pred_df = pred_df.merge(model_locations, on='places')\n",
        "pred_df = pred_df.rename(columns={'train_test_y':'train_test'})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60xDNqo3zghm"
      },
      "source": [
        "\n",
        "# categorize rows that contain a known PFAS release\n",
        "pred_df.loc[(pred_df['pred_result'] == 'True Positive') | (pred_df['pred_result'] == 'False Negative'), 'Release'] = True\n",
        "pred_df.loc[(pred_df['pred_result'] == 'True Negative') | (pred_df['pred_result'] == 'False Positive'), 'Release'] = False\n",
        "\n",
        "pred_df.loc[pred_df['pred_result'] == 'True Positive', 'Prediction Outcome'] = 'Release(s) Occurred:  Correctly Predicted'\n",
        "pred_df.loc[pred_df['pred_result'] == 'False Positive', 'Prediction Outcome'] = 'No Documented Release:  Higher Risk Predicted'\n",
        "pred_df.loc[pred_df['pred_result'] == 'True Negative', 'Prediction Outcome'] = 'No Documented Release:  Lower Risk Predicted'\n",
        "pred_df.loc[pred_df['pred_result'] == 'False Negative', 'Prediction Outcome'] = 'Release(s) Occurred:  Model Failed to Predict'\n",
        "\n",
        "pred_df.loc[(pred_df['Release'] == True) & (pred_df['train_test'] == 'Training Set'), 'train_test_release'] = 'Training Locations: Release(s) Occurred'\n",
        "pred_df.loc[(pred_df['Release'] == True) & (pred_df['train_test'] == 'Test Set'), 'train_test_release'] = 'Test Locations: Release(s) Occurred'\n",
        "pred_df.loc[(pred_df['Release'] == False) & (pred_df['train_test'] == 'Training Set'), 'train_test_release'] = 'Test Locations: No Documented Release'\n",
        "pred_df.loc[(pred_df['Release'] == False) & (pred_df['train_test'] == 'Test Set'), 'train_test_release'] = 'Training Locations: No Documented Release'\n",
        "\n",
        "\n",
        "# merge prediction data with gdf\n",
        "places_gdf = places_gdf.merge(pred_df, on='places')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dov9GtwEuVIs"
      },
      "source": [
        "\n",
        "# export datafiles \n",
        "places_gdf.to_file('Release risk model area places geodataframe.shp')  \n",
        "pred_df.to_csv('Release Risk model predictions data.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWVoTUtGJ0kN"
      },
      "source": [
        "\n",
        "# import locations of documented PFAS release\n",
        "\n",
        "ds_locations = pd.read_parquet('../../data/disposal_sites/PFAS_Sites_2021-11-07_geocoded.parquet')\n",
        "\n",
        "# mismatch between ds_locations and places_gdf\n",
        "ds_locations = ds_locations.drop(16, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmbtDZtTJDfG"
      },
      "source": [
        "\n",
        "# Where have PFAS Releases occurred\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "                  ds_locations,\n",
        "                  lat='lat', lon='lon', \n",
        "                  hover_data=['RTN','Town','Site_Name','Address','Notif_Date','Disposition','Chemical'],\n",
        "                  mapbox_style='carto-positron',   \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=1,            \n",
        "                  title='Locations of Documented PFAS Release'\n",
        "              )\n",
        "\n",
        "fig.update_traces(marker={'size': 8, \n",
        "                          'color':'#b2182b'})\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1000,                \n",
        "  )\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': False}\n",
        "fig.show(config=config)\n",
        "\n",
        "# save html output\n",
        "if config['staticPlot'] == False:\n",
        "  fig.write_html(\"PFAS release locations scatterplot map.html\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJFsR3GUGt_"
      },
      "source": [
        "\n",
        "# Where have PFAS Releases occurred\n",
        "\n",
        "fig = px.choropleth_mapbox(\n",
        "                  places_gdf,\n",
        "                  geojson=places_gdf['geometry'], \n",
        "                  locations=places_gdf.index, \n",
        "                  color=\"Release\",\n",
        "                  color_discrete_map={True:'#b2182b', # dark red\n",
        "                                      False:'lightgray'},\n",
        "                  mapbox_style='carto-positron',\n",
        "                  # mapbox_style='white-bg',             \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=0.7,                  \n",
        "                  title='Areas Where PFAS Releases Have Occurred'\n",
        "              )\n",
        "\n",
        "\n",
        "fig2 = px.scatter_mapbox(\n",
        "                  ds_locations,\n",
        "                  lat='lat', lon='lon', \n",
        "                  hover_data=[ds_locations.index, 'RTN', 'Site_Name', 'Town'],\n",
        "                  mapbox_style='carto-positron',          \n",
        "              )\n",
        "fig2.update_traces(marker={'size': 8, 'color':'black'})\n",
        "fig.add_trace(fig2.data[0])\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1000,                \n",
        "  )\n",
        "\n",
        "fig.update_layout({\n",
        "    'legend_title_text': 'Release Occurrence'},\n",
        "    title=dict(font = dict(size = 18)), \n",
        "    legend=dict(font = dict(size = 18), \n",
        "                      orientation='h'), \n",
        "    legend_title=dict(font = dict(size = 18)))\n",
        "\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': True}\n",
        "fig.show(config=config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvQS0rlPXCga"
      },
      "source": [
        "\n",
        "# What areas were used for training the model\n",
        "\n",
        "temp_gdf = places_gdf.loc[places_gdf['train_test']=='Training Set', ]\n",
        "\n",
        "fig = px.choropleth_mapbox(\n",
        "                  temp_gdf,\n",
        "                  geojson=temp_gdf['geometry'], \n",
        "                  locations=temp_gdf.index, \n",
        "                  color=\"Release\",\n",
        "                  color_discrete_map={True:'#b2182b',\n",
        "                                      False:'lightgray'},\n",
        "                  mapbox_style='carto-positron',\n",
        "                  # mapbox_style='white-bg',            \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=0.7,                  \n",
        "                  title='Model Training/Validation Regions'\n",
        "              )\n",
        "\n",
        "fig2 = px.scatter_mapbox(\n",
        "                  ds_locations,\n",
        "                  lat='lat', lon='lon', \n",
        "                  mapbox_style='carto-positron',          \n",
        "              )\n",
        "fig2.update_traces(marker={'size': 8, 'color':'black'})\n",
        "fig.add_trace(fig2.data[0])\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1000,                \n",
        "  )\n",
        "\n",
        "fig.update_layout({\n",
        "    'legend_title_text': 'Documented PFAS Release Occurred'},\n",
        "    title=dict(font = dict(size = 18)), \n",
        "    legend=dict(font = dict(size = 18), \n",
        "                      orientation='h'), \n",
        "    legend_title=dict(font = dict(size = 18)))\n",
        "\n",
        "\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': True}\n",
        "fig.show(config=config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anbXuJeg9dRk"
      },
      "source": [
        "\n",
        "# What areas were used for testing the model\n",
        "\n",
        "temp_gdf = places_gdf.loc[places_gdf['train_test']=='Test Set', ]\n",
        "\n",
        "fig = px.choropleth_mapbox(\n",
        "                  temp_gdf,\n",
        "                  geojson=temp_gdf['geometry'], \n",
        "                  locations=temp_gdf.index, \n",
        "                  color=\"Release\",\n",
        "                  color_discrete_map={True:'#b2182b',\n",
        "                                      False:'lightgray'},\n",
        "                  mapbox_style='carto-positron',\n",
        "                  # mapbox_style='white-bg',            \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=0.7,                  \n",
        "                  title='Model Testing Regions'\n",
        "              )\n",
        "fig2 = px.scatter_mapbox(\n",
        "                  ds_locations,\n",
        "                  lat='lat', lon='lon', \n",
        "                  mapbox_style='carto-positron',          \n",
        "              )\n",
        "fig2.update_traces(marker={'size': 6, 'color':'black'})\n",
        "fig.add_trace(fig2.data[0])\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1000,                \n",
        "  )\n",
        "\n",
        "fig.update_layout({\n",
        "    'legend_title_text': 'Documented PFAS<br>Release Occurred'},\n",
        "    title=dict(font = dict(size = 18)), \n",
        "    legend=dict(font = dict(size = 18), \n",
        "                      orientation='h'), \n",
        "    legend_title=dict(font = dict(size = 18)))\n",
        "\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': True}\n",
        "fig.show(config=config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7QYiYR1Vxze"
      },
      "source": [
        "\n",
        "# plot choropleth of prediction outcomes for positive response areas\n",
        "\n",
        "temp_gdf = places_gdf.loc[places_gdf['Release']==True, ]\n",
        "\n",
        "fig = px.choropleth_mapbox(\n",
        "                  temp_gdf,\n",
        "                  geojson=temp_gdf['geometry'], \n",
        "                  locations=temp_gdf.index, \n",
        "                  color=\"Prediction Outcome\",\n",
        "                  labels={'Prediction Outcome':'Model Prediction Outcome'},\n",
        "                  color_discrete_map={'Release(s) Occurred:  Correctly Predicted': '#b2182b', \n",
        "                                      'No Documented Release:  Higher Risk Predicted':'#b2182b',  \n",
        "                                      'No Documented Release:  Lower Risk Predicted': '#bababa', \n",
        "                                      'Release(s) Occurred:  Model Failed to Predict':'#404040', \n",
        "                                      },\n",
        "                  mapbox_style='carto-positron',\n",
        "                  # mapbox_style='white-bg',             \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=0.7,                  \n",
        "                  title='Model Prediction Outcomes:  Areas of Documented PFAS Release'\n",
        "              )\n",
        "\n",
        "fig2 = px.scatter_mapbox(\n",
        "                  ds_locations,\n",
        "                  lat='lat', lon='lon', \n",
        "                  mapbox_style='carto-positron',          \n",
        "              )\n",
        "fig2.update_traces(marker={'size': 6, 'color':'black'})\n",
        "fig.add_trace(fig2.data[0])\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1000,                \n",
        "  )\n",
        "\n",
        "fig.update_layout({\n",
        "    'legend_title_text': ''}, \n",
        "    title=dict(font = dict(size = 18)), \n",
        "    legend=dict(font = dict(size = 18), \n",
        "                      orientation='h'), \n",
        "    legend_title=dict(font = dict(size = 18)))\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': True}\n",
        "fig.show(config=config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH8GOnrK-pIu"
      },
      "source": [
        "\n",
        "# plot choropleth of prediction outcomes for negative response areas\n",
        "\n",
        "temp_gdf = places_gdf.loc[places_gdf['Release']==False, ]\n",
        "\n",
        "fig = px.choropleth_mapbox(\n",
        "                  temp_gdf,\n",
        "                  geojson=temp_gdf['geometry'], \n",
        "                  locations=temp_gdf.index, \n",
        "                  color=\"Prediction Outcome\",\n",
        "                  labels={'Prediction Outcome':'Model Prediction Outcome'},\n",
        "                  color_discrete_map={'Release(s) Occurred:  Correctly Predicted': '#ca0020', \n",
        "                                      'No Documented Release:  Higher Risk Predicted':'#feb24c',\n",
        "                                      'No Documented Release:  Lower Risk Predicted': '#bababa', \n",
        "                                      'Release(s) Occurred:  Model Failed to Predict':'#404040', \n",
        "                                      },\n",
        "                  mapbox_style='carto-positron',\n",
        "                  # mapbox_style='white-bg',                  \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=0.9,                  \n",
        "                  title='Model Prediction Outcomes:  Areas Without Documented PFAS Release'\n",
        "              )\n",
        "\n",
        "# fig2 = px.scatter_mapbox(\n",
        "#                   ds_locations,\n",
        "#                   lat='lat', lon='lon', \n",
        "#                   mapbox_style='carto-positron',          \n",
        "#               )\n",
        "# fig2.update_traces(marker={'size': 6, 'color':'black'})\n",
        "# fig.add_trace(fig2.data[0])\n",
        "\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1000,                \n",
        "  )\n",
        "fig.update_layout({\n",
        "    'legend_title_text': ''},\n",
        "    title=dict(font = dict(size = 18)), \n",
        "    legend=dict(font = dict(size = 18), \n",
        "                      orientation='h'), \n",
        "    legend_title=dict(font = dict(size = 18)))\n",
        "\n",
        "\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': True}\n",
        "fig.show(config=config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAkGRjGMIrgF"
      },
      "source": [
        "\n",
        "# plot choropleth of predicted probability of release\n",
        "\n",
        "temp_gdf = places_gdf.loc[places_gdf['Release']==False, ]\n",
        "places_gdf['prob'] = np.round(places_gdf['prob'], 2)\n",
        "\n",
        "fig = px.choropleth_mapbox(\n",
        "                  places_gdf,\n",
        "                  geojson=places_gdf['geometry'], \n",
        "                  locations=places_gdf.index, \n",
        "                  color=\"prob\",\n",
        "                  hover_data=['Release','Prediction Outcome', 'prob'],\n",
        "                  color_continuous_scale=\"reds\",\n",
        "                  mapbox_style='carto-positron',\n",
        "                  # mapbox_style='white-bg',                  \n",
        "                  zoom=7.4,\n",
        "                  center={'lat': 42.05, 'lon': -71.6},\n",
        "                  opacity=0.7,  \n",
        "                  title='PFAS Release Risk: Predicted Probability'\n",
        "              )\n",
        "\n",
        "fig2 = px.scatter_mapbox(\n",
        "                  ds_locations,\n",
        "                  lat='lat', lon='lon', \n",
        "                  mapbox_style='carto-positron',          \n",
        "              )\n",
        "fig2.update_traces(marker={'size': 6, 'color':'black'})\n",
        "fig.add_trace(fig2.data[0])\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(font = dict(size = 18)), \n",
        "    legend=dict(font = dict(size = 18), \n",
        "                      orientation='h'), \n",
        "    legend_title=dict(font = dict(size = 18)))\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    height=700,\n",
        "    width=1100,\n",
        "    coloraxis_colorbar={\n",
        "        'title':'Predicted<br>Probability'})\n",
        "\n",
        "\n",
        "\n",
        "# set to False to enable interactive features: hover, scroll, zoom.  \n",
        "config = {'staticPlot': True}\n",
        "fig.show(config=config)\n",
        "\n",
        "          \n",
        "fig.write_html(\"Release Risk Model Risk Probability Map.html\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations for presenting"
      ],
      "metadata": {
        "id": "7FYKoFjZV1OK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1nDLKcbXRJv"
      },
      "source": [
        "\n",
        "# plot prediction results \n",
        "\n",
        "plot_df = pred_df.sort_values('prob', ascending=True)\n",
        "plot_df = plot_df.reset_index(drop=True)\n",
        "\n",
        "font = {'size'   : 18}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "pal = {'Release(s) Occurred:  Correctly Predicted': 'green',    \n",
        "       'No Documented Release:  Higher Risk Predicted':'red',  \n",
        "       'No Documented Release:  Lower Risk Predicted': '#bababa',  \n",
        "       'Release(s) Occurred:  Model Failed to Predict':'black'}  \n",
        "\n",
        "plt.axhline(.5, linestyle='--', linewidth=1, color='green', alpha=.6)\n",
        "sns.scatterplot(plot_df.index, plot_df['prob'], hue=plot_df['Prediction Outcome'], palette=pal, s=60, alpha=.8)\n",
        "\n",
        "plt.ylim(0,1), \n",
        "plt.yticks(np.arange(0, 1.1, .1))\n",
        "plt.grid(which='major', axis='y', linewidth=.3)\n",
        "\n",
        "plt.xlabel('Locations - ordered index')\n",
        "plt.ylabel('Predicted Probability')\n",
        "plt.title('Model Prediction Outcomes')\n",
        "# plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', markerscale=1.5);  # right side\n",
        "plt.legend(bbox_to_anchor=(-0.03, -.2), loc='upper left', markerscale=1.5); # below chart\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "pal = {'Release(s) Occurred:  Correctly Predicted': 'green',    # dark red\n",
        "       'No Documented Release:  Higher Risk Predicted':'red',  # orange\n",
        "       'No Documented Release:  Lower Risk Predicted': '#bababa',  # light gray\n",
        "       'Release(s) Occurred:  Model Failed to Predict':'black'}  # dark gray\n",
        "\n",
        "# plot prediction outcome totals \n",
        "ax = sns.countplot(data=pred_df, \n",
        "                   y='Prediction Outcome', \n",
        "                   order=['Release(s) Occurred:  Correctly Predicted',\n",
        "                          'No Documented Release:  Higher Risk Predicted',\n",
        "                          'No Documented Release:  Lower Risk Predicted',\n",
        "                          'Release(s) Occurred:  Model Failed to Predict'],\n",
        "                   palette=pal)\n",
        "\n",
        "\n",
        "\n",
        "plt.bar_label(ax.containers[0])\n",
        "plt.xlim(0,30)\n",
        "plt.ylabel('')\n",
        "plt.title('')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "54zhp5XUOBqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}